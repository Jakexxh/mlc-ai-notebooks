{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn-Ig7wiqkGz"
      },
      "source": [
        "# Add a New Model Architecture in MLC-LLM using SLM workflow\n",
        "\n",
        "In this tutorial, we will demonstrate how to add a new model architecture in MLC-LLM using the new SLM workflow. SLM is the new model compilation workflow to bring modularized Python-first compilation to MLC-LLM, allowing users and developers to support new models and features more seamlessly.\n",
        "\n",
        "As an example, under SLM, the amount of code required to define a Mistral model architecture is only about half of that under the old workflow.\n",
        "\n",
        "But we still recommend reading through the [old tutorial](https://github.com/mlc-ai/notebooks/blob/main/tutorial/How_to_add_model_architeture_in_MLC_LLM.ipynb) to have some background understanding of the TVM Unity core and TensorIR.\n",
        "\n",
        "Here, we are going to use [GPT-2](https://huggingface.co/gpt2) for demonstration purpose. GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion, which can be used to guess the next word in sentences. It's model definition in Huggingface can be found [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py).\n",
        "\n",
        "Learn more about MLC LLM here: https://mlc.ai/mlc-llm/docs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4D8NYIHqkG0"
      },
      "source": [
        "Click the button below to get started:\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_add_new_model_architecture_in_SLM.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpRXEjhVqkG0"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "We will start from setting up the environment. First, let us create a new Conda environment, in which we will run the rest of the notebook.\n",
        "\n",
        "```bash\n",
        "conda create --name mlc-llm python=3.10\n",
        "conda activate mlc-llm\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCWv5nc2qkG0"
      },
      "source": [
        "**Google Colab**\n",
        "- If you are running this in a Google Colab notebook, you would not need to create a conda environment.\n",
        "- However, be sure to change your runtime to GPU by going to `Runtime` > `Change runtime type` and setting the Hardware accelerator to be \"GPU\".\n",
        "- Besides, compiling GPT-2 **may** require more RAM than the default Colab allocates. You may need to either upgrade Colab to a paid plan (so that `runtime shape` can be set to `High RAM`), or use other environments.\n",
        "  - But we also notice that, sometimes rerunning it several times (just the build portion) without exceeding the default RAM amount."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYdAwIcFqkG0"
      },
      "source": [
        "If you are using CUDA, you can run the following command to confirm that CUDA is set up correctly, and check the driver version number as well as what GPUs are currently available for use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSR5pScmqkG1",
        "outputId": "df28515c-3a8c-46f9-ed60-94064f517897"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec 28 01:58:01 2023       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwl92tl4qkG1"
      },
      "source": [
        "Next, let's download the MLC-AI and MLC-Chat nightly build packages. If you are running in a Colab environment, then you can just run the following command. Otherwise, go to https://mlc.ai/package/ and replace the command below with the one that is appropriate for your hardware and OS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsGNOvZHqkG1"
      },
      "source": [
        "**Google Colab**: If you are using Colab, you may see the red warnings such as **\"You must restart the runtime in order to use newly installed versions.\"** For our purpose, we can disregard them, the notebook will still run correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y900W-JIqkG1",
        "outputId": "f0d934a4-6a8e-4f10-e0e7-27ae843a4f27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://mlc.ai/wheels\n",
            "Collecting mlc-ai-nightly-cu122\n",
            "  Downloading https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_ai_nightly_cu122-0.12.dev1940-cp310-cp310-manylinux_2_28_x86_64.whl (471.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.9/471.9 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mlc-chat-nightly-cu122\n",
            "  Downloading https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_chat_nightly_cu122-0.1.dev712-cp310-cp310-manylinux_2_28_x86_64.whl (65.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting attrs (from mlc-ai-nightly-cu122)\n",
            "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "Collecting cloudpickle (from mlc-ai-nightly-cu122)\n",
            "  Using cached cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
            "Collecting decorator (from mlc-ai-nightly-cu122)\n",
            "  Using cached decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting ml-dtypes (from mlc-ai-nightly-cu122)\n",
            "  Using cached ml_dtypes-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
            "Collecting numpy (from mlc-ai-nightly-cu122)\n",
            "  Using cached numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "Collecting psutil (from mlc-ai-nightly-cu122)\n",
            "  Using cached psutil-5.9.7-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (285 kB)\n",
            "Collecting scipy (from mlc-ai-nightly-cu122)\n",
            "  Using cached scipy-1.12.0rc1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "Collecting tornado (from mlc-ai-nightly-cu122)\n",
            "  Using cached tornado-6.4-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
            "Collecting typing-extensions (from mlc-ai-nightly-cu122)\n",
            "  Using cached typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Collecting fastapi (from mlc-chat-nightly-cu122)\n",
            "  Using cached fastapi-0.108.0-py3-none-any.whl (92 kB)\n",
            "Collecting uvicorn (from mlc-chat-nightly-cu122)\n",
            "  Using cached uvicorn-0.25.0-py3-none-any.whl (60 kB)\n",
            "Collecting shortuuid (from mlc-chat-nightly-cu122)\n",
            "  Using cached shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 (from fastapi->mlc-chat-nightly-cu122)\n",
            "  Using cached pydantic-2.5.3-py3-none-any.whl (381 kB)\n",
            "Collecting starlette<0.33.0,>=0.29.0 (from fastapi->mlc-chat-nightly-cu122)\n",
            "  Using cached starlette-0.32.0.post1-py3-none-any.whl (70 kB)\n",
            "Collecting click>=7.0 (from uvicorn->mlc-chat-nightly-cu122)\n",
            "  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
            "Collecting h11>=0.8 (from uvicorn->mlc-chat-nightly-cu122)\n",
            "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->mlc-chat-nightly-cu122)\n",
            "  Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Collecting pydantic-core==2.14.6 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->mlc-chat-nightly-cu122)\n",
            "  Using cached pydantic_core-2.14.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "Collecting anyio<5,>=3.4.0 (from starlette<0.33.0,>=0.29.0->fastapi->mlc-chat-nightly-cu122)\n",
            "  Using cached anyio-4.2.0-py3-none-any.whl (85 kB)\n",
            "Collecting idna>=2.8 (from anyio<5,>=3.4.0->starlette<0.33.0,>=0.29.0->fastapi->mlc-chat-nightly-cu122)\n",
            "  Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
            "Collecting sniffio>=1.1 (from anyio<5,>=3.4.0->starlette<0.33.0,>=0.29.0->fastapi->mlc-chat-nightly-cu122)\n",
            "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting exceptiongroup>=1.0.2 (from anyio<5,>=3.4.0->starlette<0.33.0,>=0.29.0->fastapi->mlc-chat-nightly-cu122)\n",
            "  Using cached exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: typing-extensions, tornado, sniffio, shortuuid, psutil, numpy, idna, h11, exceptiongroup, decorator, cloudpickle, click, attrs, annotated-types, uvicorn, scipy, pydantic-core, ml-dtypes, anyio, starlette, pydantic, mlc-ai-nightly-cu122, fastapi, mlc-chat-nightly-cu122\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.9.0\n",
            "    Uninstalling typing_extensions-4.9.0:\n",
            "      Successfully uninstalled typing_extensions-4.9.0\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 6.4\n",
            "    Uninstalling tornado-6.4:\n",
            "      Successfully uninstalled tornado-6.4\n",
            "  Attempting uninstall: sniffio\n",
            "    Found existing installation: sniffio 1.3.0\n",
            "    Uninstalling sniffio-1.3.0:\n",
            "      Successfully uninstalled sniffio-1.3.0\n",
            "  Attempting uninstall: shortuuid\n",
            "    Found existing installation: shortuuid 1.0.11\n",
            "    Uninstalling shortuuid-1.0.11:\n",
            "      Successfully uninstalled shortuuid-1.0.11\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.7\n",
            "    Uninstalling psutil-5.9.7:\n",
            "      Successfully uninstalled psutil-5.9.7\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.2\n",
            "    Uninstalling numpy-1.26.2:\n",
            "      Successfully uninstalled numpy-1.26.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.6\n",
            "    Uninstalling idna-3.6:\n",
            "      Successfully uninstalled idna-3.6\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: exceptiongroup\n",
            "    Found existing installation: exceptiongroup 1.2.0\n",
            "    Uninstalling exceptiongroup-1.2.0:\n",
            "      Successfully uninstalled exceptiongroup-1.2.0\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 5.1.1\n",
            "    Uninstalling decorator-5.1.1:\n",
            "      Successfully uninstalled decorator-5.1.1\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 3.0.0\n",
            "    Uninstalling cloudpickle-3.0.0:\n",
            "      Successfully uninstalled cloudpickle-3.0.0\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.7\n",
            "    Uninstalling click-8.1.7:\n",
            "      Successfully uninstalled click-8.1.7\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 23.1.0\n",
            "    Uninstalling attrs-23.1.0:\n",
            "      Successfully uninstalled attrs-23.1.0\n",
            "  Attempting uninstall: annotated-types\n",
            "    Found existing installation: annotated-types 0.6.0\n",
            "    Uninstalling annotated-types-0.6.0:\n",
            "      Successfully uninstalled annotated-types-0.6.0\n",
            "  Attempting uninstall: uvicorn\n",
            "    Found existing installation: uvicorn 0.25.0\n",
            "    Uninstalling uvicorn-0.25.0:\n",
            "      Successfully uninstalled uvicorn-0.25.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: SciPy 1.12.0rc1\n",
            "    Uninstalling SciPy-1.12.0rc1:\n",
            "      Successfully uninstalled SciPy-1.12.0rc1\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.14.6\n",
            "    Uninstalling pydantic_core-2.14.6:\n",
            "      Successfully uninstalled pydantic_core-2.14.6\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.3.1\n",
            "    Uninstalling ml-dtypes-0.3.1:\n",
            "      Successfully uninstalled ml-dtypes-0.3.1\n",
            "  Attempting uninstall: anyio\n",
            "    Found existing installation: anyio 4.2.0\n",
            "    Uninstalling anyio-4.2.0:\n",
            "      Successfully uninstalled anyio-4.2.0\n",
            "  Attempting uninstall: starlette\n",
            "    Found existing installation: starlette 0.32.0.post1\n",
            "    Uninstalling starlette-0.32.0.post1:\n",
            "      Successfully uninstalled starlette-0.32.0.post1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.5.3\n",
            "    Uninstalling pydantic-2.5.3:\n",
            "      Successfully uninstalled pydantic-2.5.3\n",
            "  Attempting uninstall: mlc-ai-nightly-cu122\n",
            "    Found existing installation: mlc-ai-nightly-cu122 0.12.dev1940\n",
            "    Uninstalling mlc-ai-nightly-cu122-0.12.dev1940:\n",
            "      Successfully uninstalled mlc-ai-nightly-cu122-0.12.dev1940\n",
            "  Attempting uninstall: fastapi\n",
            "    Found existing installation: fastapi 0.108.0\n",
            "    Uninstalling fastapi-0.108.0:\n",
            "      Successfully uninstalled fastapi-0.108.0\n",
            "  Attempting uninstall: mlc-chat-nightly-cu122\n",
            "    Found existing installation: mlc-chat-nightly-cu122 0.1.dev712\n",
            "    Uninstalling mlc-chat-nightly-cu122-0.1.dev712:\n",
            "      Successfully uninstalled mlc-chat-nightly-cu122-0.1.dev712\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "google-colab 1.0.0 requires tornado==6.3.2, but you have tornado 6.4 which is incompatible.\n",
            "jupyter-server 1.24.0 requires anyio<4,>=3.1.0, but you have anyio 4.2.0 which is incompatible.\n",
            "moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\n",
            "tensorflow 2.15.0 requires ml-dtypes~=0.2.0, but you have ml-dtypes 0.3.1 which is incompatible.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed annotated-types-0.6.0 anyio-4.2.0 attrs-23.1.0 click-8.1.7 cloudpickle-3.0.0 decorator-5.1.1 exceptiongroup-1.2.0 fastapi-0.108.0 h11-0.14.0 idna-3.6 ml-dtypes-0.3.1 mlc-ai-nightly-cu122-0.12.dev1940 mlc-chat-nightly-cu122-0.1.dev712 numpy-1.26.2 psutil-5.9.7 pydantic-2.5.3 pydantic-core-2.14.6 scipy-1.12.0rc1 shortuuid-1.0.11 sniffio-1.3.0 starlette-0.32.0.post1 tornado-6.4 typing-extensions-4.9.0 uvicorn-0.25.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "decorator",
                  "psutil",
                  "tornado"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install --pre --force-reinstall mlc-ai-nightly-cu122 mlc-chat-nightly-cu122 -f https://mlc.ai/wheels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Google Colab**: Since we ignored the warnings/errors in the previous cell, run the following cell to verify the installation did in fact occur properly."
      ],
      "metadata": {
        "id": "ERZhUPnzrQqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import tvm; print('tvm installed properly!')\"\n",
        "!python -c \"import mlc_chat; print('mlc_chat installed properly!')\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITS3dagXrW7g",
        "outputId": "94054e8c-add3-47fe-a794-59e65250133a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tvm installed properly!\n",
            "mlc_chat installed properly!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we clone the [mlc-llm repository](https://github.com/mlc-ai/mlc-llm).\n",
        "\n",
        "**Google Colab**: Note, this will install into the mlc-llm folder. You can click the folder icon on the left menu bar to see the local file system and verify that the repository was cloned successfully."
      ],
      "metadata": {
        "id": "RxH-11nWrdoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --recursive https://github.com/mlc-ai/mlc-llm.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpSOTqKnrwMU",
        "outputId": "874afbc1-3d4e-4377-9117-93c029413955"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'mlc-llm' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then install `mlc-llm` as a package, so that we can use its functions outside of this directory."
      ],
      "metadata": {
        "id": "AkQLvRcFsAyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd mlc-llm && pip install -e . && cd -"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYtxJI_bsCS6",
        "outputId": "c0d6c3f8-4807-47fa-f7bd-b0c028a22176"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/mlc-llm\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mlc_llm==0.1.dev714+g09ec207) (1.26.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mlc_llm==0.1.dev714+g09ec207) (2.1.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from mlc_llm==0.1.dev714+g09ec207) (4.35.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from mlc_llm==0.1.dev714+g09ec207) (1.12.0rc1)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from mlc_llm==0.1.dev714+g09ec207) (0.9.12)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm->mlc_llm==0.1.dev714+g09ec207) (0.16.0+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm->mlc_llm==0.1.dev714+g09ec207) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm->mlc_llm==0.1.dev714+g09ec207) (0.19.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->mlc_llm==0.1.dev714+g09ec207) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mlc_llm==0.1.dev714+g09ec207) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->mlc_llm==0.1.dev714+g09ec207) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->mlc_llm==0.1.dev714+g09ec207) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mlc_llm==0.1.dev714+g09ec207) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mlc_llm==0.1.dev714+g09ec207) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->mlc_llm==0.1.dev714+g09ec207) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->mlc_llm==0.1.dev714+g09ec207) (2.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->mlc_llm==0.1.dev714+g09ec207) (23.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mlc_llm==0.1.dev714+g09ec207) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->mlc_llm==0.1.dev714+g09ec207) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->mlc_llm==0.1.dev714+g09ec207) (0.15.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->mlc_llm==0.1.dev714+g09ec207) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mlc_llm==0.1.dev714+g09ec207) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mlc_llm==0.1.dev714+g09ec207) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mlc_llm==0.1.dev714+g09ec207) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mlc_llm==0.1.dev714+g09ec207) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mlc_llm==0.1.dev714+g09ec207) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->mlc_llm==0.1.dev714+g09ec207) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm->mlc_llm==0.1.dev714+g09ec207) (9.4.0)\n",
            "Building wheels for collected packages: mlc_llm\n",
            "  Building editable for mlc_llm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mlc_llm: filename=mlc_llm-0.1.dev714+g09ec207-0.editable-py3-none-any.whl size=7384 sha256=17a4f12bfeb1c0f0c416b4e1fc138df38c9dcd7db3b008f7d90ce9573721384e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_66don6s/wheels/60/f6/e4/f9ebad71d5663623c41caead0eb5663a07b045d94af8e40d00\n",
            "Successfully built mlc_llm\n",
            "Installing collected packages: mlc_llm\n",
            "  Attempting uninstall: mlc_llm\n",
            "    Found existing installation: mlc_llm 0.1.dev714+g09ec207\n",
            "    Uninstalling mlc_llm-0.1.dev714+g09ec207:\n",
            "      Successfully uninstalled mlc_llm-0.1.dev714+g09ec207\n",
            "Successfully installed mlc_llm-0.1.dev714+g09ec207\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the GPT-2 Model\n",
        "\n",
        "Create a `gpt2` folder under `mlc-llm/python/mlc_chat/model/`. It's structure will look like the following:\n",
        "\n",
        "```\n",
        "mlc-llm/python/mlc_chat/model/gpt2/\n",
        "├── gpt2_loader.py          # Load and convert the weights from Huggingface\n",
        "├── gpt2_model.py           # Define the model architecture and configuration\n",
        "├── gpt2_quantization.py    # Define quantization schemes\n",
        "└── __init__.py\n",
        "```\n",
        "\n",
        "We first focus on `gpt2_model.py`. This file defines the GPT-2 model architecture in a modularized fashion using `tvm.relax.frontend.nn.Module`, similar to the PyTorch counterpart."
      ],
      "metadata": {
        "id": "IAp_SpW6sN2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a Config Class in gpt2_model.py\n",
        "\n",
        "Let's first define a config class that is almost a direct translation from Huggingface's [GPT2Config](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/configuration_gpt2.py). The attributes of this class should have the same name as the corresponding attributes in the huggingface config, otherwise, the huggingface config won't be loaded properly.\n",
        "\n",
        "The `__post_init__` function is called after all the dataclass attributes are initialized."
      ],
      "metadata": {
        "id": "_-O_-R7rsN4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dataclasses\n",
        "import math\n",
        "\n",
        "from mlc_chat.support.config import ConfigBase\n",
        "\n",
        "from tvm import te, tir\n",
        "from tvm.relax.frontend import nn\n",
        "from tvm.relax.frontend.nn import Tensor, op\n",
        "from typing import Any, Dict, Optional\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class GPT2Config(ConfigBase):  # pylint: disable=too-many-instance-attributes\n",
        "    \"\"\"Configuration of the GPT-2 model.\"\"\"\n",
        "\n",
        "    vocab_size: int\n",
        "    n_embd: int\n",
        "    n_layer: int\n",
        "    n_head: int\n",
        "    layer_norm_epsilon: int\n",
        "    n_inner: int = -1\n",
        "    scale_attn_by_inverse_layer_idx: bool = False\n",
        "    # Internal configs used by MLC-LLM\n",
        "    context_window_size: int = 0\n",
        "    prefill_chunk_size: int = 0\n",
        "    kwargs: Dict[str, Any] = dataclasses.field(default_factory=dict)\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.n_inner is None or self.n_inner == -1:\n",
        "            self.n_inner = 4 * self.n_embd\n",
        "\n",
        "        self.context_window_size = self.kwargs[\"n_positions\"]\n",
        "\n",
        "        # Internal configs initialization"
      ],
      "metadata": {
        "id": "z8xwX363wsqx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define model architecture in gpt2_model.py\n",
        "\n",
        "With `tvm.relax.frontend.nn.Module`, we are able to define the model architecture in a modularized fashion. It looks pretty similar to the PyTorch style, except that the forward function does not actually perform the computation. It traces the operator graph using the placeholders that are passed as inputs.\n",
        "\n",
        "Here we only present the GPT2Attention module. The entire model definition can be found [here](https://github.com/mlc-ai/mlc-llm/blob/main/python/mlc_chat/compiler/model/gpt2/gpt2_model.py)."
      ],
      "metadata": {
        "id": "50zWSA0ksN7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Attention(nn.Module):\n",
        "    def __init__(self, config, layer_idx: int=None):\n",
        "        self.embed_dim = config.n_embd\n",
        "        self.num_heads = config.n_head\n",
        "        self.head_dim = self.embed_dim // self.num_heads\n",
        "        self.scale_attn_by_inverse_layer_idx = config.scale_attn_by_inverse_layer_idx\n",
        "        self.layer_idx = layer_idx\n",
        "\n",
        "        self.c_attn = nn.Linear(\n",
        "            in_features=self.embed_dim,\n",
        "            out_features=3 * self.num_heads * self.head_dim,\n",
        "            bias=True,\n",
        "        )\n",
        "        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n",
        "\n",
        "        self.k_cache = nn.KVCache(config.context_window_size, [self.num_heads, self.head_dim])\n",
        "        self.v_cache = nn.KVCache(config.context_window_size, [self.num_heads, self.head_dim])\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: Tensor,\n",
        "        attention_mask: Tensor,\n",
        "    ):\n",
        "        d, h, t = self.head_dim, self.num_heads, 2\n",
        "        b, s, _ = hidden_states.shape\n",
        "        assert b == 1, \"Only support batch size 1 at this moment.\"\n",
        "\n",
        "        qkv = self.c_attn(hidden_states)\n",
        "        qkv = op.reshape(qkv, (b, s, 3 * h, d))\n",
        "        q, k, v = op.split(qkv, 3, axis=2)\n",
        "\n",
        "        self.k_cache.append(op.squeeze(k, axis=0))\n",
        "        self.v_cache.append(op.squeeze(v, axis=0))\n",
        "        k = op.reshape(self.k_cache.view(t), (b, t, h, d))\n",
        "        v = op.reshape(self.v_cache.view(t), (b, t, h, d))\n",
        "\n",
        "        q = q.permute_dims([0, 2, 1, 3])  # [b, h, s, d]\n",
        "        k = k.permute_dims([0, 2, 1, 3])  # [b, h, t, d]\n",
        "        v = v.permute_dims([0, 2, 1, 3])  # [b, h, t, d]\n",
        "\n",
        "        attn_weights = op.matmul(\n",
        "            q, k.permute_dims([0, 1, 3, 2])  # [b, h, s, d] x [b, h, d, t] = [b, h, s, t]\n",
        "        ) / math.sqrt(d)\n",
        "\n",
        "        if self.scale_attn_by_inverse_layer_idx:\n",
        "            attn_weights = attn_weights / float(self.layer_idx + 1)\n",
        "\n",
        "        dtype = attn_weights.dtype\n",
        "        if attention_mask is not None:\n",
        "            attn_weights = attn_weights.maximum(tir.min_value(dtype)).minimum(attention_mask)\n",
        "\n",
        "        if dtype == \"float32\":\n",
        "            attn_weights = op.softmax(attn_weights, axis=-1)\n",
        "        else:\n",
        "            attn_weights = op.softmax(attn_weights.astype(\"float32\"), axis=-1).astype(dtype)\n",
        "        # [b, h, s, t] x [b, h, t, d] => [b, h, s, d] => [b, s, h, d]\n",
        "        output = op.matmul(attn_weights, v)\n",
        "        return self.c_proj(output.permute_dims([0, 2, 1, 3]).reshape((b, s, h * d)))"
      ],
      "metadata": {
        "id": "fh_6l1Ul1sWA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we have already provided some built-in common modules that you will find handy. For example, the `nn.Linear` and `nn.KVCache` modules here are all built-in modules in MLC-LLM. A full list of built-in modules can be found [here](https://github.com/apache/tvm/blob/unity/python/tvm/relax/frontend/nn/modules.py).\n",
        "\n",
        "Similarly, we have also provided a lot of common built-in operations that operates on the Tensors. For example, `op.reshape`, `op.matmul`, `op.softmax`, etc.A full list of built-in operations can be found [here](https://github.com/apache/tvm/blob/unity/python/tvm/relax/frontend/nn/op.py)."
      ],
      "metadata": {
        "id": "2prefTaZsN9q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validating the Correctness of an `nn.Module`\n",
        "\n",
        "Once you finished defining an `nn.Module`, you can compare it against its Huggingface PyTorch counterpart to make sure it behaves correctly. We can do so by:\n",
        "- Initialize an MLC `nn.Module` from the class we just defined\n",
        "- Load the corresponding module in Huggingface PyTorch model\n",
        "- Copy the parameter weights from Huggingface module to MLC module\n",
        "- Use `jit` to provide a TVM run-time that converts the MLC module to a PyTorch-compatible runnable module\n",
        "- Feed the same PyTorch tensor as input to both modules and compare the output"
      ],
      "metadata": {
        "id": "sv5h6EwpsOCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Initialize an MLC `nn.Module` from the class we just defined\n",
        "\n",
        "from tvm.relax.frontend.nn import spec\n",
        "\n",
        "config_dict = {\n",
        "    \"architectures\": [\"GPT2LMHeadModel\"],\n",
        "    \"bos_token_id\": 50256,\n",
        "    \"eos_token_id\": 50256,\n",
        "    \"hidden_act\": \"gelu_new\",\n",
        "    \"n_ctx\": 1024,\n",
        "    \"n_embd\": 768,\n",
        "    \"n_head\": 12,\n",
        "    \"n_layer\": 12,\n",
        "    \"n_positions\": 1024,\n",
        "    \"layer_norm_epsilon\": 1e-05,\n",
        "    \"scale_attn_by_inverse_layer_idx\": False,\n",
        "    \"vocab_size\": 50257,\n",
        "}\n",
        "\n",
        "attn_spec = {\"forward\": {\"hidden_states\": spec.Tensor([1, 2, 768], dtype=\"float32\"), \"attention_mask\": spec.Tensor([1, 1, 2, 2], dtype=\"float32\")}}\n",
        "\n",
        "config = GPT2Config.from_dict(config_dict)\n",
        "mlc_attn = GPT2Attention(config ,layer_idx=5)"
      ],
      "metadata": {
        "id": "3Xoyo4_w_jQK"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we have also defined a JSON like dictionary as ModuleSpec, which describes how the placeholders in the module's forward function are defined. For example, here we define the hidden_states to be of shape [1, 2, 768], which corresponds to [batch_size, total_sequence_length, n_embd].\n",
        "\n",
        "Now, we can export this module to TVM IRModule and parameters, and we can do a sanity check on the shape and data type of the parameters."
      ],
      "metadata": {
        "id": "HIRFy_1NB88A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mod, named_params = mlc_attn.export_tvm(spec=attn_spec)\n",
        "\n",
        "for name, param in named_params:\n",
        "    print(name, param.shape, param.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3w8rv1JoB-bb",
        "outputId": "73d72331-40fb-46ee-a0f9-b45da7e0aa30"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c_attn.weight [2304, 768] float32\n",
            "c_attn.bias [2304] float32\n",
            "c_proj.weight [768, 768] float32\n",
            "c_proj.bias [768] float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load the corresponding module in Huggingface PyTorch model\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "hf_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "hf_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qin9I35d7uqN",
        "outputId": "8f55cb2c-5f6d-4d41-f86d-4b68ceb2bfe3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hf_attn = hf_model.transformer.h[5].attn"
      ],
      "metadata": {
        "id": "1qn2G2Dy9mOM"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Copy the parameter weights from Huggingface module to MLC module\n",
        "\n",
        "hf_state_dict = hf_attn.state_dict()\n",
        "new_state_dict = {}\n",
        "\n",
        "# Transpose the weight in attention layer since Huggingface implementation uses Conv1D instead of Linear\n",
        "for k, v in hf_state_dict.items():\n",
        "    if \"weight\" in k:\n",
        "        new_state_dict[k] = v.T\n",
        "    else:\n",
        "        new_state_dict[k] = v\n",
        "\n",
        "mlc_attn.load_state_dict(new_state_dict, strict=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvrrYxPu-BWB",
        "outputId": "80dc6468-253e-46c6-8d5c-116bb856c268"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([], [])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Use jit to provide a TVM run-time that converts the MLC module to a PyTorch-compatible runnable module\n",
        "\n",
        "torch_attn = mlc_attn.jit(spec=attn_spec, device=\"cpu\")"
      ],
      "metadata": {
        "id": "HwHhZyPSAnoS"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Feed the same PyTorch tensor as input to both modules and compare the output\n",
        "\n",
        "import torch\n",
        "\n",
        "x = torch.rand((1, 2, 768), dtype=torch.float32)\n",
        "\n",
        "mask = torch.full((1, 1, 2, 2), torch.finfo(torch.float32).max, dtype=torch.float32)\n",
        "mask[0, 0, 0, 1] = torch.finfo(torch.float32).min\n",
        "\n",
        "hf_y = hf_attn.forward(x)    # In Huggingface attention implementation, causal mask is automatically applied\n",
        "mlc_y = torch_attn[\"forward\"](x, mask)\n",
        "assert torch.allclose(hf_y[0], mlc_y, atol=1e-5)"
      ],
      "metadata": {
        "id": "6I-2G0zDDIL5"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a Loader in gpt2_loader.py\n",
        "\n",
        "In `gpt2_loader.py`, we define how we convert the parameters from Huggingface to the format used by MLC model.\n",
        "\n",
        "The loader class will return an [`ExternMapping`](https://github.com/mlc-ai/mlc-llm/blob/main/python/mlc_chat/loader/mapping.py) that contains two kinds of mappings:\n",
        "- Source -> MLC parameter mapping: for example, parameter renaming, parameter transformation, etc.\n",
        "- Unused mapping: parameters in the source that are not used in the MLC model definition.\n",
        "\n",
        "In GPT2, we need to transpose c_attn, c_proj and c_fc weights since GPT-2 uses Conv1D. To do so, we will supply a mapping function as follows\n",
        "\n",
        "```\n",
        "for conv1d_weight_name in [\"attn.c_attn\", \"attn.c_proj\", \"mlp.c_proj\", \"mlp.c_fc\"]:\n",
        "    src_name = f\"h.{i}.{conv1d_weight_name}.weight\"\n",
        "    mlc_name = f\"transformer.{src_name}\"\n",
        "    mapping.add_mapping(\n",
        "        mlc_name,\n",
        "        [src_name],\n",
        "        functools.partial(\n",
        "            lambda x, dtype: x.transpose().astype(dtype),\n",
        "            dtype=named_parameters[mlc_name].dtype,\n",
        "        ),\n",
        "    )\n",
        "```\n",
        "\n",
        "Some renamings are also needed for GPT-2 parameters conversion to work. Please refer to [gpt2_loader.py](https://github.com/mlc-ai/mlc-llm/blob/main/python/mlc_chat/model/gpt2/gpt2_loader.py)."
      ],
      "metadata": {
        "id": "iR7ijB5WNiML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add the Model to the Supported Pre-built Model Workflow\n",
        "\n",
        "Once the entire model is defined in SLM, including the model architecture, model loader and model quantitizer, we can then add it to the supported pre-built model workflow.\n",
        "\n",
        "In [`mlc-llm/python/mlc_chat/model/model.py`](https://github.com/mlc-ai/mlc-llm/blob/main/python/mlc_chat/model/model.py), add the GPT-2 model to the `MODELS` list:\n",
        "\n",
        "```\n",
        "\"gpt2\": Model(\n",
        "    name=\"gpt2\",\n",
        "    model=gpt2_model.GPT2LMHeadModel,\n",
        "    config=gpt2_model.GPT2Config,\n",
        "    source={\n",
        "        \"huggingface-torch\": gpt2_loader.huggingface,\n",
        "        \"huggingface-safetensor\": gpt2_loader.huggingface,\n",
        "    },\n",
        "    quantize={\n",
        "        \"no-quant\": gpt2_quantization.no_quant,\n",
        "        \"group-quant\": gpt2_quantization.group_quant,\n",
        "    },\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "Nekb6Ku4V3F2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compile GPT-2 model libraries and weights\n",
        "\n",
        "The following steps will be the same as the general model compilation workflow [here](https://llm.mlc.ai/docs/compilation/compile_models.html)."
      ],
      "metadata": {
        "id": "AwsQF8zEYh8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create directory\n",
        "!mkdir -p dist/models && cd dist/models\n",
        "\n",
        "# Clone HF weights\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/gpt2\n",
        "!cd ../.."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWLOmEXyZV9q",
        "outputId": "8793e771-59bf-48d3-8bc4-327cdfd08c21"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into 'gpt2'...\n",
            "remote: Enumerating objects: 84, done.\u001b[K\n",
            "remote: Counting objects: 100% (84/84), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 84 (delta 31), reused 69 (delta 28), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (84/84), 1.66 MiB | 3.81 MiB/s, done.\n",
            "Filtering content: 100% (11/11), 5.23 GiB | 30.99 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert weight\n",
        "!mlc_chat convert_weight ./dist/models/gpt2/ --quantization q4f16_1 -o dist/gpt2-q4f16_1-MLC"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzN9QbUpYeot",
        "outputId": "c4625286-72e5-4191-ee1e-785076eeda67"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-28 03:12:59] INFO utils.py:160: NumExpr defaulting to 2 threads.\n",
            "[2023-12-28 03:13:02] INFO auto_config.py:116: \u001b[92mFound\u001b[0m model configuration: dist/models/gpt2/config.json\n",
            "[2023-12-28 03:13:02] INFO auto_device.py:75: \u001b[92mFound\u001b[0m device: cuda:0\n",
            "[2023-12-28 03:13:03] INFO auto_device.py:84: \u001b[91mNot found\u001b[0m device: rocm:0\n",
            "[2023-12-28 03:13:03] INFO auto_device.py:84: \u001b[91mNot found\u001b[0m device: metal:0\n",
            "[2023-12-28 03:13:04] INFO auto_device.py:84: \u001b[91mNot found\u001b[0m device: vulkan:0\n",
            "[2023-12-28 03:13:04] INFO auto_device.py:84: \u001b[91mNot found\u001b[0m device: opencl:0\n",
            "[2023-12-28 03:13:04] INFO auto_device.py:33: Using device: \u001b[1mcuda:0\u001b[0m\n",
            "[2023-12-28 03:13:04] INFO auto_weight.py:70: Finding weights in: dist/models/gpt2\n",
            "[2023-12-28 03:13:04] INFO auto_weight.py:129: \u001b[92mFound\u001b[0m source weight format: huggingface-torch. Source configuration: dist/models/gpt2/pytorch_model.bin\n",
            "[2023-12-28 03:13:04] INFO auto_weight.py:149: \u001b[91mNot found\u001b[0m Huggingface Safetensor\n",
            "[2023-12-28 03:13:04] INFO auto_weight.py:106: Using source weight configuration: \u001b[1mdist/models/gpt2/pytorch_model.bin\u001b[0m. Use `--source` to override.\n",
            "[2023-12-28 03:13:04] INFO auto_weight.py:110: Using source weight format: \u001b[1mhuggingface-torch\u001b[0m. Use `--source-format` to override.\n",
            "[2023-12-28 03:13:04] INFO auto_config.py:155: \u001b[92mFound\u001b[0m model type: \u001b[1mgpt2\u001b[0m. Use `--model-type` to override.\n",
            "\u001b[1mWeight conversion with arguments:\u001b[0m\n",
            "  \u001b[1m--config\u001b[0m          dist/models/gpt2/config.json\n",
            "  \u001b[1m--quantization\u001b[0m    GroupQuantize(name='q4f16_1', kind='group-quant', group_size=32, quantize_dtype='int4', storage_dtype='uint32', model_dtype='float16', num_elem_per_storage=8, num_storage_per_group=4, max_int_value=7)\n",
            "  \u001b[1m--model-type\u001b[0m      gpt2\n",
            "  \u001b[1m--device\u001b[0m          cuda:0\n",
            "  \u001b[1m--source\u001b[0m          dist/models/gpt2/pytorch_model.bin\n",
            "  \u001b[1m--source-format\u001b[0m   huggingface-torch\n",
            "  \u001b[1m--output\u001b[0m          dist/gpt2-q4f16_1-MLC\n",
            "[2023-12-28 03:13:04] INFO gpt2_model.py:44: \u001b[1mcontext_window_size\u001b[0m not found in config.json. Falling back to \u001b[1mn_positions\u001b[0m (1024)\n",
            "[2023-12-28 03:13:05] INFO huggingface_loader.py:169: Loading HF parameters from: dist/models/gpt2/pytorch_model.bin\n",
            "[2023-12-28 03:13:10] INFO group_quantization.py:199: Compiling quantize function for key: (50257, 768, 'float16', 'cuda')\n",
            "[2023-12-28 03:13:13] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mlm_head.q_weight\u001b[0m\", shape: (50257, 96), dtype: uint32\n",
            "[2023-12-28 03:13:13] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mlm_head.q_scale\u001b[0m\", shape: (50257, 24), dtype: float16\n",
            "[2023-12-28 03:13:14] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.wte.q_weight\u001b[0m\", shape: (50257, 96), dtype: uint32\n",
            "[2023-12-28 03:13:14] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.wte.q_scale\u001b[0m\", shape: (50257, 24), dtype: float16\n",
            "[2023-12-28 03:13:14] INFO group_quantization.py:199: Compiling quantize function for key: (1024, 768, 'float16', 'cuda')\n",
            "[2023-12-28 03:13:14] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.wpe.q_weight\u001b[0m\", shape: (1024, 96), dtype: uint32\n",
            "[2023-12-28 03:13:14] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.wpe.q_scale\u001b[0m\", shape: (1024, 24), dtype: float16\n",
            "[2023-12-28 03:13:14] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.0.ln_1.weight\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:14] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.0.ln_1.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:14] INFO group_quantization.py:199: Compiling quantize function for key: (2304, 768, 'float16', 'cuda')\n",
            "[2023-12-28 03:13:15] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.0.attn.c_attn.q_weight\u001b[0m\", shape: (2304, 96), dtype: uint32\n",
            "[2023-12-28 03:13:15] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.0.attn.c_attn.q_scale\u001b[0m\", shape: (2304, 24), dtype: float16\n",
            "[2023-12-28 03:13:15] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.0.attn.c_attn.bias\u001b[0m\", shape: (2304,), dtype: float16\n",
            "[2023-12-28 03:13:15] INFO group_quantization.py:199: Compiling quantize function for key: (768, 768, 'float16', 'cuda')\n",
            "[2023-12-28 03:13:16] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.0.attn.c_proj.q_weight\u001b[0m\", shape: (768, 96), dtype: uint32\n",
            "[2023-12-28 03:13:16] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.0.attn.c_proj.q_scale\u001b[0m\", shape: (768, 24), dtype: float16\n",
            "[2023-12-28 03:13:16] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.0.attn.c_proj.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:16] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.0.ln_2.weight\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:16] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.0.ln_2.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:16] INFO group_quantization.py:199: Compiling quantize function for key: (3072, 768, 'float16', 'cuda')\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.0.mlp.c_fc.q_weight\u001b[0m\", shape: (3072, 96), dtype: uint32\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.0.mlp.c_fc.q_scale\u001b[0m\", shape: (3072, 24), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.0.mlp.c_fc.bias\u001b[0m\", shape: (3072,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO group_quantization.py:199: Compiling quantize function for key: (768, 3072, 'float16', 'cuda')\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.0.mlp.c_proj.q_weight\u001b[0m\", shape: (768, 384), dtype: uint32\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.0.mlp.c_proj.q_scale\u001b[0m\", shape: (768, 96), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.0.mlp.c_proj.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.1.ln_1.weight\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.1.ln_1.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.1.attn.c_attn.q_weight\u001b[0m\", shape: (2304, 96), dtype: uint32\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.1.attn.c_attn.q_scale\u001b[0m\", shape: (2304, 24), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.1.attn.c_attn.bias\u001b[0m\", shape: (2304,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.1.attn.c_proj.q_weight\u001b[0m\", shape: (768, 96), dtype: uint32\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.1.attn.c_proj.q_scale\u001b[0m\", shape: (768, 24), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.1.attn.c_proj.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.1.ln_2.weight\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.1.ln_2.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.1.mlp.c_fc.q_weight\u001b[0m\", shape: (3072, 96), dtype: uint32\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.1.mlp.c_fc.q_scale\u001b[0m\", shape: (3072, 24), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.1.mlp.c_fc.bias\u001b[0m\", shape: (3072,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.1.mlp.c_proj.q_weight\u001b[0m\", shape: (768, 384), dtype: uint32\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.1.mlp.c_proj.q_scale\u001b[0m\", shape: (768, 96), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.1.mlp.c_proj.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.2.ln_1.weight\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.2.ln_1.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.2.attn.c_attn.q_weight\u001b[0m\", shape: (2304, 96), dtype: uint32\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.2.attn.c_attn.q_scale\u001b[0m\", shape: (2304, 24), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.2.attn.c_attn.bias\u001b[0m\", shape: (2304,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.2.attn.c_proj.q_weight\u001b[0m\", shape: (768, 96), dtype: uint32\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.2.attn.c_proj.q_scale\u001b[0m\", shape: (768, 24), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.2.attn.c_proj.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.2.ln_2.weight\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.2.ln_2.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.2.mlp.c_fc.q_weight\u001b[0m\", shape: (3072, 96), dtype: uint32\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.2.mlp.c_fc.q_scale\u001b[0m\", shape: (3072, 24), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.2.mlp.c_fc.bias\u001b[0m\", shape: (3072,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.2.mlp.c_proj.q_weight\u001b[0m\", shape: (768, 384), dtype: uint32\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.2.mlp.c_proj.q_scale\u001b[0m\", shape: (768, 96), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.2.mlp.c_proj.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.3.ln_1.weight\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.3.ln_1.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.3.attn.c_attn.q_weight\u001b[0m\", shape: (2304, 96), dtype: uint32\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.3.attn.c_attn.q_scale\u001b[0m\", shape: (2304, 24), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.3.attn.c_attn.bias\u001b[0m\", shape: (2304,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.3.attn.c_proj.q_weight\u001b[0m\", shape: (768, 96), dtype: uint32\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.3.attn.c_proj.q_scale\u001b[0m\", shape: (768, 24), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.3.attn.c_proj.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.3.ln_2.weight\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.3.ln_2.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.3.mlp.c_fc.q_weight\u001b[0m\", shape: (3072, 96), dtype: uint32\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.3.mlp.c_fc.q_scale\u001b[0m\", shape: (3072, 24), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.3.mlp.c_fc.bias\u001b[0m\", shape: (3072,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.3.mlp.c_proj.q_weight\u001b[0m\", shape: (768, 384), dtype: uint32\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.3.mlp.c_proj.q_scale\u001b[0m\", shape: (768, 96), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.3.mlp.c_proj.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.4.ln_1.weight\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.4.ln_1.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.4.attn.c_attn.q_weight\u001b[0m\", shape: (2304, 96), dtype: uint32\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.4.attn.c_attn.q_scale\u001b[0m\", shape: (2304, 24), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.4.attn.c_attn.bias\u001b[0m\", shape: (2304,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.4.attn.c_proj.q_weight\u001b[0m\", shape: (768, 96), dtype: uint32\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.4.attn.c_proj.q_scale\u001b[0m\", shape: (768, 24), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.4.attn.c_proj.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.4.ln_2.weight\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.4.ln_2.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.4.mlp.c_fc.q_weight\u001b[0m\", shape: (3072, 96), dtype: uint32\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.4.mlp.c_fc.q_scale\u001b[0m\", shape: (3072, 24), dtype: float16\n",
            "[2023-12-28 03:13:17] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.4.mlp.c_fc.bias\u001b[0m\", shape: (3072,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.4.mlp.c_proj.q_weight\u001b[0m\", shape: (768, 384), dtype: uint32\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.4.mlp.c_proj.q_scale\u001b[0m\", shape: (768, 96), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.4.mlp.c_proj.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.5.ln_1.weight\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.5.ln_1.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.5.attn.c_attn.q_weight\u001b[0m\", shape: (2304, 96), dtype: uint32\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.5.attn.c_attn.q_scale\u001b[0m\", shape: (2304, 24), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.5.attn.c_attn.bias\u001b[0m\", shape: (2304,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.5.attn.c_proj.q_weight\u001b[0m\", shape: (768, 96), dtype: uint32\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.5.attn.c_proj.q_scale\u001b[0m\", shape: (768, 24), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.5.attn.c_proj.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.5.ln_2.weight\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.5.ln_2.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.5.mlp.c_fc.q_weight\u001b[0m\", shape: (3072, 96), dtype: uint32\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.5.mlp.c_fc.q_scale\u001b[0m\", shape: (3072, 24), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.5.mlp.c_fc.bias\u001b[0m\", shape: (3072,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.5.mlp.c_proj.q_weight\u001b[0m\", shape: (768, 384), dtype: uint32\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.5.mlp.c_proj.q_scale\u001b[0m\", shape: (768, 96), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.5.mlp.c_proj.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.6.ln_1.weight\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.6.ln_1.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.6.attn.c_attn.q_weight\u001b[0m\", shape: (2304, 96), dtype: uint32\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.6.attn.c_attn.q_scale\u001b[0m\", shape: (2304, 24), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.6.attn.c_attn.bias\u001b[0m\", shape: (2304,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.6.attn.c_proj.q_weight\u001b[0m\", shape: (768, 96), dtype: uint32\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.6.attn.c_proj.q_scale\u001b[0m\", shape: (768, 24), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.6.attn.c_proj.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.6.ln_2.weight\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.6.ln_2.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.6.mlp.c_fc.q_weight\u001b[0m\", shape: (3072, 96), dtype: uint32\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.6.mlp.c_fc.q_scale\u001b[0m\", shape: (3072, 24), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.6.mlp.c_fc.bias\u001b[0m\", shape: (3072,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.6.mlp.c_proj.q_weight\u001b[0m\", shape: (768, 384), dtype: uint32\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.6.mlp.c_proj.q_scale\u001b[0m\", shape: (768, 96), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.6.mlp.c_proj.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.7.ln_1.weight\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.7.ln_1.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.7.attn.c_attn.q_weight\u001b[0m\", shape: (2304, 96), dtype: uint32\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.7.attn.c_attn.q_scale\u001b[0m\", shape: (2304, 24), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.7.attn.c_attn.bias\u001b[0m\", shape: (2304,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.7.attn.c_proj.q_weight\u001b[0m\", shape: (768, 96), dtype: uint32\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.7.attn.c_proj.q_scale\u001b[0m\", shape: (768, 24), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.7.attn.c_proj.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.7.ln_2.weight\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.7.ln_2.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.7.mlp.c_fc.q_weight\u001b[0m\", shape: (3072, 96), dtype: uint32\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.7.mlp.c_fc.q_scale\u001b[0m\", shape: (3072, 24), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.7.mlp.c_fc.bias\u001b[0m\", shape: (3072,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.7.mlp.c_proj.q_weight\u001b[0m\", shape: (768, 384), dtype: uint32\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.7.mlp.c_proj.q_scale\u001b[0m\", shape: (768, 96), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.7.mlp.c_proj.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.8.ln_1.weight\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.8.ln_1.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.8.attn.c_attn.q_weight\u001b[0m\", shape: (2304, 96), dtype: uint32\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.8.attn.c_attn.q_scale\u001b[0m\", shape: (2304, 24), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.8.attn.c_attn.bias\u001b[0m\", shape: (2304,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.8.attn.c_proj.q_weight\u001b[0m\", shape: (768, 96), dtype: uint32\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.8.attn.c_proj.q_scale\u001b[0m\", shape: (768, 24), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.8.attn.c_proj.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.8.ln_2.weight\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.8.ln_2.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.8.mlp.c_fc.q_weight\u001b[0m\", shape: (3072, 96), dtype: uint32\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.8.mlp.c_fc.q_scale\u001b[0m\", shape: (3072, 24), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.8.mlp.c_fc.bias\u001b[0m\", shape: (3072,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.8.mlp.c_proj.q_weight\u001b[0m\", shape: (768, 384), dtype: uint32\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.8.mlp.c_proj.q_scale\u001b[0m\", shape: (768, 96), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.8.mlp.c_proj.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.9.ln_1.weight\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.9.ln_1.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.9.attn.c_attn.q_weight\u001b[0m\", shape: (2304, 96), dtype: uint32\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.9.attn.c_attn.q_scale\u001b[0m\", shape: (2304, 24), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.9.attn.c_attn.bias\u001b[0m\", shape: (2304,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.9.attn.c_proj.q_weight\u001b[0m\", shape: (768, 96), dtype: uint32\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.9.attn.c_proj.q_scale\u001b[0m\", shape: (768, 24), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.9.attn.c_proj.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.9.ln_2.weight\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.9.ln_2.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.9.mlp.c_fc.q_weight\u001b[0m\", shape: (3072, 96), dtype: uint32\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.9.mlp.c_fc.q_scale\u001b[0m\", shape: (3072, 24), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.9.mlp.c_fc.bias\u001b[0m\", shape: (3072,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.9.mlp.c_proj.q_weight\u001b[0m\", shape: (768, 384), dtype: uint32\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.9.mlp.c_proj.q_scale\u001b[0m\", shape: (768, 96), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.9.mlp.c_proj.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.10.ln_1.weight\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.10.ln_1.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.10.attn.c_attn.q_weight\u001b[0m\", shape: (2304, 96), dtype: uint32\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.10.attn.c_attn.q_scale\u001b[0m\", shape: (2304, 24), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.10.attn.c_attn.bias\u001b[0m\", shape: (2304,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.10.attn.c_proj.q_weight\u001b[0m\", shape: (768, 96), dtype: uint32\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.10.attn.c_proj.q_scale\u001b[0m\", shape: (768, 24), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.10.attn.c_proj.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.10.ln_2.weight\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.10.ln_2.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.10.mlp.c_fc.q_weight\u001b[0m\", shape: (3072, 96), dtype: uint32\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.10.mlp.c_fc.q_scale\u001b[0m\", shape: (3072, 24), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.10.mlp.c_fc.bias\u001b[0m\", shape: (3072,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.10.mlp.c_proj.q_weight\u001b[0m\", shape: (768, 384), dtype: uint32\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.10.mlp.c_proj.q_scale\u001b[0m\", shape: (768, 96), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.10.mlp.c_proj.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.11.ln_1.weight\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.11.ln_1.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.11.attn.c_attn.q_weight\u001b[0m\", shape: (2304, 96), dtype: uint32\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.11.attn.c_attn.q_scale\u001b[0m\", shape: (2304, 24), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.11.attn.c_attn.bias\u001b[0m\", shape: (2304,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.11.attn.c_proj.q_weight\u001b[0m\", shape: (768, 96), dtype: uint32\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.11.attn.c_proj.q_scale\u001b[0m\", shape: (768, 24), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.11.attn.c_proj.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.11.ln_2.weight\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.11.ln_2.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.11.mlp.c_fc.q_weight\u001b[0m\", shape: (3072, 96), dtype: uint32\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.11.mlp.c_fc.q_scale\u001b[0m\", shape: (3072, 24), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.11.mlp.c_fc.bias\u001b[0m\", shape: (3072,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.11.mlp.c_proj.q_weight\u001b[0m\", shape: (768, 384), dtype: uint32\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:121: [Quantized] Parameter: \"\u001b[1mtransformer.h.11.mlp.c_proj.q_scale\u001b[0m\", shape: (768, 96), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.h.11.mlp.c_proj.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.ln_f.weight\u001b[0m\", shape: (768,), dtype: float16\n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:129: [Not quantized] Parameter: \"\u001b[1mtransformer.ln_f.bias\u001b[0m\", shape: (768,), dtype: float16\n",
            "100% 149/149 [00:08<00:00, 17.78it/s] \n",
            "[2023-12-28 03:13:18] INFO huggingface_loader.py:179: Unloading HF weight file: dist/models/gpt2/pytorch_model.bin\n",
            "[2023-12-28 03:13:18] INFO stats.py:71: \u001b[92mTime usage\u001b[0m: HF loading: 5.186 sec; Pre-quantization mapping: 1.039 sec; Quantization: 6.547 sec\n",
            "[2023-12-28 03:13:18] INFO stats.py:85: \u001b[92mRAM usage\u001b[0m: Peak RAM: 0.510 GB. Total bytes loaded from disk: 0.510 GB\n",
            "[2023-12-28 03:13:18] INFO convert_weight.py:109: \u001b[92mParameter size\u001b[0m after quantization: 0.086 GB\n",
            "[2023-12-28 03:13:18] INFO convert_weight.py:114: \u001b[92mTotal parameters\u001b[0m: 163,037,184\n",
            "[2023-12-28 03:13:18] INFO convert_weight.py:115: \u001b[92mBits per parameter\u001b[0m: 4.509\n",
            "Start storing to cache dist/gpt2-q4f16_1-MLC\n",
            "[0200/0200] saving transformer.ln_f.bias\n",
            "All finished, 4 total shards committed, record saved to dist/gpt2-q4f16_1-MLC/ndarray-cache.json\n",
            "[2023-12-28 03:13:19] INFO convert_weight.py:131: Saved to directory: \u001b[1mdist/gpt2-q4f16_1-MLC\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. gen_config: generate mlc-chat-config.json and process tokenizers\n",
        "!mlc_chat gen_config ./dist/models/gpt2 \\\n",
        "    --quantization q4f16_1 --conv-template gpt2 \\\n",
        "    -o dist/gpt2-q4f16_1-MLC/\n",
        "\n",
        "# 2. compile: compile model library with specification in mlc-chat-config.json\n",
        "!mlc_chat compile ./dist/gpt2-q4f16_1-MLC/mlc-chat-config.json \\\n",
        "    --device cuda -o dist/gpt2-q4f16_1-MLC/gpt2-q4f16_1-cuda.so"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJg-pMTGab2c",
        "outputId": "547cec16-9a6a-4674-b2cd-220f295c99e3"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-28 03:22:31] INFO utils.py:160: NumExpr defaulting to 2 threads.\n",
            "[2023-12-28 03:22:32] INFO auto_config.py:116: \u001b[92mFound\u001b[0m model configuration: dist/models/gpt2/config.json\n",
            "[2023-12-28 03:22:32] INFO auto_config.py:155: \u001b[92mFound\u001b[0m model type: \u001b[1mgpt2\u001b[0m. Use `--model-type` to override.\n",
            "[2023-12-28 03:22:32] INFO gpt2_model.py:44: \u001b[1mcontext_window_size\u001b[0m not found in config.json. Falling back to \u001b[1mn_positions\u001b[0m (1024)\n",
            "[2023-12-28 03:22:32] INFO gen_config.py:114: [generation_config.json] Setting \u001b[1mbos_token_id\u001b[0m: 50256\n",
            "[2023-12-28 03:22:32] INFO gen_config.py:114: [generation_config.json] Setting \u001b[1meos_token_id\u001b[0m: 50256\n",
            "[2023-12-28 03:22:32] INFO gen_config.py:128: \u001b[91mNot found\u001b[0m tokenizer config: dist/models/gpt2/tokenizer.model\n",
            "[2023-12-28 03:22:32] INFO gen_config.py:126: \u001b[92mFound\u001b[0m tokenizer config: dist/models/gpt2/tokenizer.json. Copying to \u001b[1mdist/gpt2-q4f16_1-MLC/tokenizer.json\u001b[0m\n",
            "[2023-12-28 03:22:32] INFO gen_config.py:126: \u001b[92mFound\u001b[0m tokenizer config: dist/models/gpt2/vocab.json. Copying to \u001b[1mdist/gpt2-q4f16_1-MLC/vocab.json\u001b[0m\n",
            "[2023-12-28 03:22:32] INFO gen_config.py:126: \u001b[92mFound\u001b[0m tokenizer config: dist/models/gpt2/merges.txt. Copying to \u001b[1mdist/gpt2-q4f16_1-MLC/merges.txt\u001b[0m\n",
            "[2023-12-28 03:22:32] INFO gen_config.py:128: \u001b[91mNot found\u001b[0m tokenizer config: dist/models/gpt2/added_tokens.json\n",
            "[2023-12-28 03:22:32] INFO gen_config.py:128: \u001b[91mNot found\u001b[0m tokenizer config: dist/models/gpt2/tokenizer_config.json\n",
            "[2023-12-28 03:22:32] INFO gen_config.py:68: [System default] Setting \u001b[1mpad_token_id\u001b[0m: 0\n",
            "[2023-12-28 03:22:32] INFO gen_config.py:68: [System default] Setting \u001b[1mtemperature\u001b[0m: 0.7\n",
            "[2023-12-28 03:22:32] INFO gen_config.py:68: [System default] Setting \u001b[1mrepetition_penalty\u001b[0m: 1.0\n",
            "[2023-12-28 03:22:32] INFO gen_config.py:68: [System default] Setting \u001b[1mtop_p\u001b[0m: 0.95\n",
            "[2023-12-28 03:22:32] INFO gen_config.py:68: [System default] Setting \u001b[1mmean_gen_len\u001b[0m: 128\n",
            "[2023-12-28 03:22:32] INFO gen_config.py:68: [System default] Setting \u001b[1mmax_gen_len\u001b[0m: 512\n",
            "[2023-12-28 03:22:32] INFO gen_config.py:68: [System default] Setting \u001b[1mshift_fill_factor\u001b[0m: 0.3\n",
            "[2023-12-28 03:22:32] INFO gen_config.py:156: Dumping configuration file to: \u001b[1mdist/gpt2-q4f16_1-MLC/mlc-chat-config.json\u001b[0m\n",
            "[2023-12-28 03:22:33] INFO utils.py:160: NumExpr defaulting to 2 threads.\n",
            "[2023-12-28 03:22:34] INFO auto_config.py:68: \u001b[92mFound\u001b[0m model configuration: dist/gpt2-q4f16_1-MLC/mlc-chat-config.json\n",
            "[2023-12-28 03:22:35] INFO auto_device.py:75: \u001b[92mFound\u001b[0m device: cuda:0\n",
            "[2023-12-28 03:22:35] INFO auto_target.py:62: \u001b[92mFound\u001b[0m configuration of target device \"\u001b[1mcuda:0\u001b[0m\": {\"thread_warp_size\": 32, \"arch\": \"sm_75\", \"max_threads_per_block\": 1024, \"max_num_threads\": 1024, \"kind\": \"cuda\", \"max_shared_memory_per_block\": 49152, \"tag\": \"\", \"keys\": [\"cuda\", \"gpu\"]}\n",
            "[2023-12-28 03:22:35] INFO auto_target.py:94: \u001b[92mFound\u001b[0m host LLVM triple: \u001b[1mx86_64-redhat-linux-gnu\u001b[0m\n",
            "[2023-12-28 03:22:35] INFO auto_target.py:95: \u001b[92mFound\u001b[0m host LLVM CPU: \u001b[1mskylake-avx512\u001b[0m\n",
            "[2023-12-28 03:22:35] INFO auto_target.py:244: Generating code for CUDA architecture: \u001b[1msm_75\u001b[0m\n",
            "[2023-12-28 03:22:35] INFO auto_target.py:245: To produce multi-arch fatbin, set environment variable \u001b[1mMLC_MULTI_ARCH\u001b[0m. Example: MLC_MULTI_ARCH=70,72,75,80,86,87,89,90\n",
            "[2023-12-28 03:22:35] INFO auto_config.py:155: \u001b[92mFound\u001b[0m model type: \u001b[1mgpt2\u001b[0m. Use `--model-type` to override.\n",
            "\u001b[1mCompiling with arguments:\u001b[0m\n",
            "  \u001b[1m--config\u001b[0m          GPT2Config(vocab_size=50257, n_embd=768, n_layer=12, n_head=12, layer_norm_epsilon=1e-05, n_inner=3072, context_window_size=1024, prefill_chunk_size=1024, scale_attn_by_inverse_layer_idx=False, tensor_parallel_shards=1, kwargs={'model_type': 'gpt2', 'quantization': 'q4f16_1', 'model_config': {'vocab_size': 50257, 'n_embd': 768, 'n_layer': 12, 'n_head': 12, 'layer_norm_epsilon': 1e-05, 'n_inner': 3072, 'context_window_size': 1024, 'prefill_chunk_size': 1024, 'scale_attn_by_inverse_layer_idx': False, 'tensor_parallel_shards': 1}, 'sliding_window_size': -1, 'attention_sink_size': -1, 'mean_gen_len': 128, 'max_gen_len': 512, 'shift_fill_factor': 0.3, 'temperature': 0.7, 'repetition_penalty': 1.0, 'top_p': 0.95, 'conv_template': 'gpt2', 'pad_token_id': 0, 'bos_token_id': 50256, 'eos_token_id': 50256, 'tokenizer_files': ['tokenizer.json', 'vocab.json', 'merges.txt'], 'version': '0.1.0'})\n",
            "  \u001b[1m--quantization\u001b[0m    GroupQuantize(name='q4f16_1', kind='group-quant', group_size=32, quantize_dtype='int4', storage_dtype='uint32', model_dtype='float16', num_elem_per_storage=8, num_storage_per_group=4, max_int_value=7)\n",
            "  \u001b[1m--model-type\u001b[0m      gpt2\n",
            "  \u001b[1m--target\u001b[0m          {\"thread_warp_size\": 32, \"host\": {\"mtriple\": \"x86_64-redhat-linux-gnu\", \"tag\": \"\", \"kind\": \"llvm\", \"mcpu\": \"skylake-avx512\", \"keys\": [\"cpu\"]}, \"arch\": \"sm_75\", \"max_threads_per_block\": 1024, \"max_num_threads\": 1024, \"kind\": \"cuda\", \"max_shared_memory_per_block\": 49152, \"tag\": \"\", \"keys\": [\"cuda\", \"gpu\"]}\n",
            "  \u001b[1m--opt\u001b[0m             flashinfer=0;cublas_gemm=1;cudagraph=0\n",
            "  \u001b[1m--system-lib-prefix\u001b[0m \"\"\n",
            "  \u001b[1m--output\u001b[0m          dist/gpt2-q4f16_1-MLC/gpt2-q4f16_1-cuda.so\n",
            "  \u001b[1m--overrides\u001b[0m       context_window_size=None;sliding_window_size=None;prefill_chunk_size=None;attention_sink_size=None;max_batch_size=None;tensor_parallel_shards=None\n",
            "[2023-12-28 03:22:35] INFO compile.py:108: Creating model from: GPT2Config(vocab_size=50257, n_embd=768, n_layer=12, n_head=12, layer_norm_epsilon=1e-05, n_inner=3072, context_window_size=1024, prefill_chunk_size=1024, scale_attn_by_inverse_layer_idx=False, tensor_parallel_shards=1, kwargs={'model_type': 'gpt2', 'quantization': 'q4f16_1', 'model_config': {'vocab_size': 50257, 'n_embd': 768, 'n_layer': 12, 'n_head': 12, 'layer_norm_epsilon': 1e-05, 'n_inner': 3072, 'context_window_size': 1024, 'prefill_chunk_size': 1024, 'scale_attn_by_inverse_layer_idx': False, 'tensor_parallel_shards': 1}, 'sliding_window_size': -1, 'attention_sink_size': -1, 'mean_gen_len': 128, 'max_gen_len': 512, 'shift_fill_factor': 0.3, 'temperature': 0.7, 'repetition_penalty': 1.0, 'top_p': 0.95, 'conv_template': 'gpt2', 'pad_token_id': 0, 'bos_token_id': 50256, 'eos_token_id': 50256, 'tokenizer_files': ['tokenizer.json', 'vocab.json', 'merges.txt'], 'version': '0.1.0'})\n",
            "[2023-12-28 03:22:35] INFO compile.py:111: Exporting the model to TVM Unity compiler\n",
            "[2023-12-28 03:22:36] INFO compile.py:117: Running optimizations using TVM Unity\n",
            "[2023-12-28 03:22:36] INFO compile.py:129: Registering metadata: {'model_type': 'gpt2', 'quantization': 'q4f16_1', 'context_window_size': 1024, 'prefill_chunk_size': 1024, 'sliding_window_size': -1, 'attention_sink_size': -1, 'tensor_parallel_shards': 1}\n",
            "[2023-12-28 03:22:36] INFO pipeline.py:33: Running TVM Relax graph-level optimizations\n",
            "[2023-12-28 03:22:36] INFO pipeline.py:33: Lowering to TVM TIR kernels\n",
            "[2023-12-28 03:22:38] INFO pipeline.py:33: Running TVM TIR-level optimizations\n",
            "[2023-12-28 03:22:40] INFO pipeline.py:33: Running TVM Dlight low-level optimizations\n",
            "[2023-12-28 03:23:00] INFO pipeline.py:33: Running memory optimizations\n",
            "[2023-12-28 03:23:01] INFO pipeline.py:33: Compiling external modules\n",
            "[2023-12-28 03:23:01] INFO pipeline.py:33: Compilation complete! Exporting to disk\n",
            "[2023-12-28 03:23:11] INFO compile.py:142: Generated: \u001b[1mdist/gpt2-q4f16_1-MLC/gpt2-q4f16_1-cuda.so\u001b[0m\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}